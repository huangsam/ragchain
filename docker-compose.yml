services:
  chroma:
    image: ghcr.io/chroma-core/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - chroma_demo_data:/data
    profiles: ["test", "demo"]
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'exec 3<>/dev/tcp/localhost/8000; echo -e \"GET /api/v2/heartbeat HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n\" >&3; cat <&3 | grep \"200 OK\"'"]
      interval: 5s
      timeout: 5s
      retries: 5

  ragchain:
    build:
      context: .
      dockerfile: Dockerfile.demo
    depends_on:
      - chroma
    profiles: ["demo"]
    environment:
      - CHROMA_SERVER_URL=http://chroma:8000
      # Point to host's Ollama instance
      - OLLAMA_HOST=http://host.docker.internal:11434
      # Default model to use (ensure you have pulled it: `ollama pull qwen3`)
      - OLLAMA_MODEL=qwen3
      # Cache HuggingFace models to a volume so we don't re-download on every restart
      - HUGGINGFACE_HUB_CACHE=/data/hf_cache
    volumes:
      - hf_cache:/data/hf_cache
    # Map to a different host port to avoid conflicts with local dev servers
    ports:
      - "8003:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/health || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 5

  demo-runner:
    build:
      context: .
      dockerfile: Dockerfile.demo
    depends_on:
      ragchain:
        condition: service_healthy
      chroma:
        condition: service_healthy
    profiles: ["demo"]
    environment:
      - RAGCHAIN_API_URL=http://ragchain:8000
      - CHROMA_SERVER_URL=http://chroma:8000
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    entrypoint: ["ragchain", "ingest", "--n", "20"]

volumes:
  chroma_demo_data:
  hf_cache:
